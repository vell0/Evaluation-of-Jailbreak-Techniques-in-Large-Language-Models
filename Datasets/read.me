Here's the raw markdown code for your datasets/README.md file:
markdown# Dataset Processing

This directory contains scripts for acquiring, processing, and generating the test cases for jailbreak evaluation. Due to licensing constraints, we do not include the actual datasets in this repository.

## Source Datasets

To reproduce our results, you'll need to obtain these datasets from their official sources:

1. **TrustAIRLab Forbidden Question Set**
   - Source: [Hugging Face](https://huggingface.co/datasets/TrustAIRLab/forbidden_question_set)
   - Contains 390 questions across 13 harmful categories
   - No explicit license provided

2. **TrustAIRLab In-The-Wild Jailbreak Prompts**
   - Source: [Hugging Face](https://huggingface.co/datasets/TrustAIRLab/in-the-wild-jailbreak-prompts)
   - Contains 1,405 jailbreak prompts (MIT License)
   - Downloaded version: 'jailbreak_2023_12_25'

## Scripts Overview

### `download_data.py`

Downloads the required datasets from Hugging Face.

```bash
# Download all required datasets
python download_data.py

# Download specific dataset
python download_data.py --dataset forbidden_questions
process_dan_prompts.py
Implements our prompt selection methodology to identify 30 representative DAN prompts.
bashpython process_dan_prompts.py --output selected_dan_prompts.csv
The selection process:

Identifies top 11 jailbreak community types
Extracts earliest and latest examples from each community
Uses sentence embeddings to find the most semantically central prompt
Applies semantic deduplication (cosine similarity threshold 0.95)

citation_generator.py
Generates domain-appropriate citations for forbidden questions.
bash# Generate all citation types
python citation_generator.py --output citations/

# Generate specific citation type
python citation_generator.py --type paper --output citations/paper/
Supported citation types:

paper: Academic papers (for professional domains)
github: GitHub repositories (for technical topics)
news: News articles
social_media: Social media posts

Test Case Construction
The complete test dataset comprises:

11,700 DAN test cases (30 prompts × 390 questions)
1,560 Citation test cases (4 citation types × 390 questions)

For each forbidden question, test cases are constructed by combining with:

Each of the 30 selected DAN prompts
Each of the 4 citation types with appropriately generated citations

Citation Mapping
We map harm categories to most effective citation types:
Harm CategoryPrimary Citation TypeSecondary TypeIllegal ActivitypaperbookHate SpeechpapernewsMalwaregithubpaperPhysical HarmpaperbookEconomic HarmnewspaperFraudgithubnewsPornographysocial_mediabookPolitical LobbyingpapernewsPrivacy ViolationgithubpaperLegal OpinionpaperbookFinancial AdvicepapernewsHealth ConsultationpaperbookGov Decisionpapernews
Usage Instructions

Download the source datasets using download_data.py
Process DAN prompts with process_dan_prompts.py
Generate citations with citation_generator.py
The main evaluation script (../main_code.py) will automatically combine these into complete test cases